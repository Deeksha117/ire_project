Topic Detection by Clustering Keywords
Christian Wartena and Rogier Brussee Telematica Instituut, P.O.
Box 589, 7500 AN Enschede, The Netherlands {Christian.Wartena,Rogier.Brussee}@telin.nl
Abstract
We consider topic detection without any prior knowledge of category structure or possible categories.
Keywords are extracted and clustered based on different similarity measures using the induced k-bisecting clustering algorithm.
Evaluation on Wikipedia articles shows that clusters of keywords correlate strongly with the Wikipedia categories of the articles.
In addition, we ﬁnd that a distance measure based on the Jensen-Shannon divergence of probability distributions outperforms the cosine similarity.
In particular, a newly proposed term distribution taking co-occurrence of terms into account gives best results.
Introduction
In this paper we consider the problem of ﬁnding the set of most prominent topics in a collection of documents.
Since we will not start with a given list of topics, we treat the problem of identifying and characterizing a topic as an integral part of the task.
As a consequence, we cannot rely on a training set or other forms of external knowledge, but have to get by with the information contained in the collection itself.
The approach we will follow consists of two steps.
First we extract a list of the most informative keywords.
Subsequently we try to identify clusters of keywords for which we will deﬁne a center, which we take as the representation of a topic.
We found that this works fairly well in an evaluation with Wikipedia articles in which we compared human deﬁned topic categories with keyword clusters.
Clustering of (key)words requires a similarity or distance measure between words.
In this paper we consider distance measures between words that are based on the statistical distribution of words in a corpus of texts.
The focus of is to ﬁnd a measure that yields good clustering results.
The organization of this paper is as follows.
In section 2 we discuss related work and the related problem of keyword extraction.
In section 3 we discuss distance functions and introduce different probability distributions needed to
deﬁne them.
We end the section with a brief description of the clustering algorithm used for evaluation.
Finally, in section 4 we present an evaluation of topic detection on a Wikipedia corpus using clustering of keywords with different distance measures.
Related Work
Much work has been done on automatic text categorization.
Most of this work is concerned with the assignment of texts onto a (small) set of given categories.
In many cases some form of machine learning is used to train an algorithm on a set of manually categorized documents.
A lot of work has been done on clustering of texts (See e.g.
[14] and [12]) which has already found practical applications like the clustering of search results (see e.g.
http: //clusty.com/).
The topic of the clusters remains usually implicit in these approaches, though it would of course be possible to apply any keyword extraction algorithm to the resulting clusters in order to ﬁnd characteristic terms.
Like the work presented in this paper, Li and Yamanishi ([10, 9]) try to ﬁnd characterizations of topics directly by clustering keywords using a statistical similarity measure.
While very similar in spirit, their similarity measure is slightly different from the Jensen-Shannon based similarity measure we use.
Moreover, they focus on determining the boundaries and the topic of short paragraphs, while we try to ﬁnd the predominant overall topic of a whole text.
Both our and their work are conceptually related to latent semantic analysis (LSA) [3, 7] and even more so to probabilistic latent semantic analysis (PLSA) [5, 6] and related work by [15].
The input for both our methods and (P)LSA is the word occurrence and co-occurrence data of terms.
Latent semantic analysis now naturally leads to weighted sums of terms, with the crucial difference that in the PLSA case only non negative weights with sum 1 are allowed which have a natural interpretation as conditional probabilities.
In the latent semantic analysis approach the analogue of clustering is ﬁnding (latent) conditional probability distributions such that we can decompose the observed word
distributions into a few of these latent distributions and a small “noisy” remainder.
In this decomposition words with strong mutual co-occurrence tend to have the same main latent components.
In our work the word clusters are similarly based on co-occurrence data.
This is achieved by comparing and clustering distributions of co-occurring terms.
Thus, the center of a cluster is the average distribution of the cooccurrence distributions, and is in this sense comparable to a latent component.
Clustering Keywords
By clustering we will understand grouping terms, documents or other items together based on some criterion for similarity.
We will always deﬁne similarity using a distance function on terms, which is in turn deﬁned as a distance between distributions associated to (key)words by counting (co)occurrences in documents.
3.1.
Keyword Extraction
We will represent a topic by a cluster of keywords.
We therefore need a set of keywords for the corpus under consideration.
Note that ﬁnding a set of keywords for a corpus is not the same problem as assigning keywords to a text from a list of keywords, nor that of ﬁnding the most characteristic terms for a given subset of the corpus.
The problem of ﬁnding a good set of keywords is similar to that of determining term weights for indexing documents, and not the main focus of this paper.
For our purposes usable results can be obtained by selecting the most frequent nouns, verbs (without auxiliaries) and proper names and ﬁltering out words with little discriminative power (see section 4).
3.2.
Distributions
We simplify a document to a bag of words, terms or keywords, in the following always called terms.
We consider a collection of n term occurrences W .
Each term occurrence is an instance of exactly one term t in T = {t1 , .
tm}, occurrences of term t in d, n(t) = P and can be found in exactly one source document d in a collection C = {d1 , .
dM }.
Let n(d, t) be the number of ber of occurrences of term t, and N (d) = P d n(d, t) be the numt n(d, t) the number of term occurrences in d. We consider the natural probability distributions Q on C × T , a distribution Q on C and q on T that measure the probability to randomly select an occurrence of a term, from a source document or both Q(d, t) = n(d, t)/n on C × T Q(d) = N (d)/n on C q(t) = n(t)/n on T
These distributions are the baseline probability distributions for everything that we will do in the remainder.
In addition we have two important conditional probabilities Q(d|t) = Qt (d) = n(d, t)/n(t) on C q(t|d) = qd (t) = n(d, t)/N (d) on T The suggestive notation Q(d|t) is used for the source distribution of t as it is the probability that a randomly selected occurrence of term t has source d. Similarly, q(t|d), the term distribution of d is the probability that a randomly selected term occurrence from document d is an instance of term t. Various other probability distributions on C × T , C and T that we will consider will be denoted by P, P , p respectively, dressed with various sub and superscripts.
Distributions of Co-occurring Terms
The setup in the previous section allows us to set up a Markov chain on the set of documents and terms which will allow us to propagate probability distributions from terms to document and vice versa.
Consider a Markov chain on T ∪C having transitions C → T with transition probabilities Q(d|t) and transitions T → C with transition probabilities q(t|d) only.
Given a term distribution p(t) we compute the one step Markov chain evolution.
This gives us a document distribution Pp (d), the probability to ﬁnd a term occurrence in a particular document given that the term distribution of the Pp (d) = X occurrences is p t Likewise given a document distribution P (d), the one step pP (t) = X Markov chain evolution is the term distribution q(t|d)P (d).
d Since P (d) gives the probability to ﬁnd a term occurrence in document d, pP is the P-weighted average of the term distributions in the documents.
Combining these, i.e.
running the Markov chain twice, every term distribution gives q(t|d)Pp (d) = X ¯p(t) = pPp (t) = X rise to a new distribution t0 ,d d In particular starting from the degenerate “known to be z” ( term distribution 1 0
q(t|d)Q(d|t0 )p(t0 )
pz (t) = p(t|z ) =
Q(d|t)p(t).
if t = z , otherwise.
q(t|d)Q(d|t0 )pz (t0 ) = X ¯pz (t) = X the distribution of co-occurring terms ¯pz then is q(t|d)Q(d|z ).
d,t0 d This distribution is the weighted average of the term distributions of documents containing z where the weight is the probability Q(d|z ) that a term occurrence in d is an occurrence of z .
Note that the probability measure ¯pz is very similar to the setup in [10, section 3].
The difference is that we keep track of the density of a keyword in a document rather than just the mere occurrence or non occurrence of a keyword in a document.
This difference is particularly relevant for long documents where a word may occur with very low density, yet because it contains many words have a signiﬁcant contribution to the mean word distribution.
Unfortunately ¯pz is expensive to compute.
3.3.
Distance Measures
An effective way to deﬁne “similarity” between two elements is through a metric d(i, j ) between the elements i, j satisfying the usual axioms of nonnegativity, identity of indiscernables and triangle inequality.
Two elements are more similar if they are closer.
For this purpose any monotone increasing function of a metric will sufﬁce and we will call such a function a distance function.
For clustering we use a hierarchical top-down method, that requires that in each step the center of each cluster is computed.
Thus our choice of distance function is restricted to distances deﬁned on a space allowing us to compute a center and distances between keywords and this center.
In particular we cannot use popular similarity measures like the Jaccard coefﬁcient.
In the following we will compare results with four different distance functions for keywords t and s: (a) the cosine similarity of the document distribution Qt and Qs considered as vectors on the document space, (b) the cosine similarity of the vectors of tf.idf values of keywords, (c) the Jensen-Shannon divergence between the document distributions Qt and Qs and (d) the Jensen-Shannon divergence between the term distributions, ¯pt and ¯ps .
The cosine similarity of two terms t and s is deﬁned as q(cid:0)P t (d)(cid:1) (cid:0)P s (d)(cid:1) .
Σd∈C Qt (d)Qs (d) d∈C Q2 d∈C Q2 Since the arccos of this similarity function is a proper metric, (1 − cos)(arccos(cos sim(t, s))) = 1 − cos sim(t, s) is a distance function.
The Jensen-Shannon divergence or information radius [11, 4] between two distributions p and q is deﬁned as 1 1 2 D(p||m) + 2 D(q ||m) JSD(p||q) =
cos simtf (t, s) =
where m = 1/2(p + q) is the mean distribution and D(p||q) is the relative entropy or Kullback-Leibler divergence be(cid:19) (cid:18) pi nX tween p and q which is deﬁned by qi i=1
D(p||q) =
pi log
Since the square root of the Jensen Shannon divergence is a proper metric [4], we have two distances
JSD simdoc (t, s) = JSD(Qt , Qs )
JSD simterm (t, s) = JSD( ¯ps , ¯pt )
3.4.
Clustering Method
We have used the induced bisecting k-means clustering algorithm as described by [1], which is based on the standard bisecting k-means algorithm, see e.g.
[14].
An informal description of the method is as follows.
We start by selecting two elements that have the largest distance, which we use as the seeds for two clusters.
Next all other items are assigned to the cluster closest to one of the two seeds.
After all items have been assigned to a cluster, the centers of both clusters are computed.
Here we need a representation of items that naturally allows to deﬁne a center which typically is not an item proper but a weighted sum of items.
The new centers serve as new seeds for ﬁnding two clusters and the process is repeated until the two centers are converged up to some predeﬁned precision.
We have now found two clusters.
If the diameter of a cluster is larger than a speciﬁed threshold value, the whole procedure is applied recursively to that cluster.
The algorithm therefore ﬁnds a binary tree of clusters.
We also experimented with agglomerative hierarchical clustering, especially the single link algorithm.
First results suggested that this approach performed not as well as the kmeans algorithm, in line with similar ﬁndings by [14] and [12].
Since our focus is to ﬁnd an optimal distance measure and not to ﬁnd the best clustering algorithm, we did not investigate the agglomerative methods in more detail.
Evaluation
4.1.
Implementation
To test and compare the different strategies we have compiled a small corpus of Dutch Wikipedia articles consisting of 758 documents.
In the analysis phase, 118099 term occurrences, and 26373 unique terms were found.
The articles were taken from 8 Wikipedia categories: spaceﬂight,
painting, architecture, trees, monocots, aviation, pop music, charadriiformes.
Categories were selected for subjective similarity, like spaceﬂight and aviation, and subjective dissimilarity like pop music and monocots.
Articles are equally distributed over the categories, but articles in some categories are signiﬁcantly longer than in others.
Moreover, homogeneity and speciﬁty of articles differs signiﬁcantly between categories.
To determine a set of relevant keywords we have selected the most frequent content words and ﬁltered out a number of overly general terms.
The latter was done by requiring that a keyword has to be different enough from the background distribution q , as measured by the Kullback-Leibler divergence.
We used a cutoff D( ¯pt ||q) > 1 bit, that turned out to give decent results.
Before extracting keywords we do some preprocessing using the GAT E–framework [2].
The main analysis steps are: lemmatization, multiword lookup and named entity recognition.
Lemmatization and tagging is done using the Treetagger [13].
Tagging allows us to distinguish content words from function words.
After lemmatization all inﬂected forms of verbs, nouns and adjectives are mapped onto their lexical form, substantially reducing the variation in the input data.
For multiword lookup we used article titles from the Dutch Wikipedia, since it is reasonable to assume that each title represents a single concept.
Finally, some simple rules are applied to identify proper names.
While some of the components are language dependent, all of the components are available for a number of languages within the GAT E–framework.
We selected the 160 most frequent keywords fulﬁlling this criterion and clustered them with the induced bisecting k-means algorithm from section 3.4 using different distance measures.
4.2.
Results
To evaluate the implemented topic detection methods, we have compared the results with topics known to be present in the collection.
We benchmarked against the 8 selected Wikipedia topics of the collection.
Of course, it is conceivable that the collection has more topics that automatic methods might recognize as well.
To deﬁne a reference clustering, we have clustered the 160 selected key9 }, one 1 , · · · , c∗ words into a set of 9 categories C ∗ = {c∗ 0 , c∗ for each Wikipedia category and a rest cluster c∗ 0 , using the following method.
For each of the 8 Wikipedia categories c∗ i we compute the distribution qc∗ of words in the documents belonging to c∗ i 0 = q .
We assign a term i and we let qc∗ z to cluster c∗ if c∗ = argminc∗∈C D(qc∗ || ¯pz ).
Following [8], we now compare with the set of clusters C of keywords found using the algorithm in section 3.4, different distance measures and different diameters.
F (c, c∗ ) =
each cluster c ∈ C and cluster c∗ ∈ C ∗ we deﬁne a recall measure rec(c, c∗ ) = |c ∩ c∗ |/|c∗ |, a precision measure prec(c, c∗ ) = |c ∩ c∗ |/|c∗ | and an F value rec(c, c∗ )prec(c, c∗ ) 2 (rec(c, c∗ ) + prec(c, c∗ )) 1 Let F (c∗ ) = maxc∈C F (c, c∗ ) be the F -value of the best F = X ﬁtting found cluster and ﬁnally deﬁne the overall F-value |c∗ |P c∗∈C ∗ |c∗ | F (c∗ ) c∗∈C ∗ A value of F = 1 therefore means that for each Wikipedia and rest category the topic detection algorithm found a corresponding cluster.
Since the clustering algorithm can produce clusters of different size, the quality of the result depends on the only input parameter of the algorithm, the threshold value to split up a cluster.
Its quality (in terms of overall F-value) is compared between distance functions by varying the number of clusters as controlled by varying the threshold values.
Since there might be several clusters with exactly the same largest distance between two elements, we do not ﬁnd values for each number of clusters.
In particular, for cos simtf and JSD simdoc there are no values that yield a clustering into less than 14 and 11 clusters, resp.
In the case of cosine similarity there are in each of these clusters even two keywords with completely orthogonal document distributions.
The overall F-values for clustering with the different similarities are given in Figure 1.
Cosine similarity with tf.idf vectors performed in between direct cosine similarity and the Jensen-Shannon distance with the document distribution The rest category in the reference categorization is not a real cluster or category of keywords.
Moreover, this category is about three times as large as the average size of the other categories.
We have therefore tested how well the 8 positively motivated categories are detected, and computed the overall F-values averaging only over these 8 categories.
The results are given in Figure 2.
Results for the cosine similarity with tf.idf vector were slightly better than clustering with JSD simdoc
Conclusions
The experimental results suggest that topic identiﬁcation by clustering a set of keywords works fairly well, using either of the investigated similarity measures.
In the present experiment a recently proposed distribution of terms associated with a keyword clearly gives best results, but computation of the distribution is relatively expensive.
The reason for this is the fact that co-occurrence of terms is (implicitly) taken into account.
The document distribution for terms,
Figure 1.
Overall F-Measure for clustering based on 9 categories
Figure 2.
Overall F-Measure for clustering based on 8 categories
that is the base of the other measures, tend to be very sparse vectors, since for a given document most words will not occur at all in that document.
In the distribution used for the JSD simdoc this problem alleviated by a kind of ’intelligent smoothing’ or spreading values from one term to frequently co-occurring terms.
Acknowledgements
This work is part of the MultimediaN project1 sponsored by the Dutch government under contract BSIK 03031.
References
[1] F. Archetti, P. Campanelli, E. Fersini, and E. Messina.
A hierarchical document clustering environment based on the induced bisecting k-means.
In H. L. Larsen, G. Pasi, D. O. Arroyo, T. Andreasen, and H. Christiansen, editors, FQAS, volume 4027 of Lecture Notes in Computer Science, pages 257–269.
Springer, 2006.
[2] H. Cunningham, D. Maynard, K. Bontcheva, and V. Tablan.
GATE: A Framework and Graphical Development EnvironIn Proc.
of ment for Robust NLP Tools and Applications.
the 40th Anniversary Meeting of the Association for Computational Linguistics (ACL’02), pages 168–175, Philadelphia, July 2002.
[3] S. T. Dumais, G. W. Furnas, T. K. Landauer, S. Deerwester, and R. Harshman.
Using latent semantic analysis to improve access to textual information.
In CHI ’88: Proceedings of the SIGCHI conference on Human factors in computing systems, pages 281–285, New York, NY, USA, 1988.
ACM.
1 http://www.multimedian.nl
[4] B. Fuglede and F. Topsoe.
Jensen-shannon divergence and hilbert space embedding.
In Proc.
of the Internat.
Symposium on Information Theory, 2004, pages 31–, 2004.
[5] T. Hofmann.
Probabilisitic latent semantic analysis.
UAI99: Uncertainty in artiﬁcial intelligence, 1999.
[6] T. Hofmann.
Unsupervised learning by probabilistic latent semantic analysis.
Machine Learning, 42(1-2):177–196, January 2001.
[7] T. Landauer, P. Foltz, and D. Laham.
Introduction to latent semantic analysis.
Discourse Processes, 25:259–284, 1998.
[8] B. Larsen and C. Aone.
Fast and effective text mining using linear-time document clustering.
In KDD, pages 16–22, 1999.
[9] H. Li and K. Yamanishi.
Text classiﬁcation using esc-based stochastic decision lists.
Inf.
Process.
Manage., 38(3):343– 361, 2002.
[10] H. Li and K. Yamanishi.
Topic analysis using a ﬁnite mixture model.
Inf.
Process.
Manage., 39(4):521–541, 2003.
[11] C. D. Manning and H. Schütze.
Foundations of Statistical Natural Language Processing.
The MIT Press, Cambridge, Massachusetts, 1999.
[12] S. Meyer zu Eissen and B. Stein.
Analysis of clustering algorithms for web-based search.
In D. Karagiannis and U. Reimer, editors, PAKM, volume 2569 of Lecture Notes in Computer Science, pages 168–178.
Springer, 2002.
[13] H. Schmid.
Probabilistic part-of-speech tagging using decision trees.
In International Conference on New Methods in Language Processing, Manchester, UK, 1994. unknown.
[14] M. Steinbach, G. Karypis, and V. Kumar.
A comparison of document clustering techniques.
In Proceedings of Workshop on Text Mining, 6th ACM SIGKDD International Conference on Data Mining (KDD’00), pages 109–110, August 20–23 2000.
[15] L. Zhang, X. Wu, and Y. Yu.
Emergent semantics from folksonomies: A quantitative study.
4090:168–186, 2006.
