206
[u'Topic Detection by Clustering Keywords\n\nChristian Wartena and Rogier Brussee\nTelematica Instituut, P.O.', u'Box 589, 7500 AN Enschede, The Netherlands\n{Christian.Wartena,Rogier.Brussee}@telin.nl\n\nAbstract\n\nWe consider topic detection without any prior knowledge\nof category structure or possible categories.', u'Keywords are\nextracted and clustered based on different similarity mea-\nsures using the induced k-bisecting clustering algorithm.', u'Evaluation on Wikipedia articles shows that clusters of key-\nwords correlate strongly with the Wikipedia categories of\nthe articles.', u'In addition, we \ufb01nd that a distance measure\nbased on the Jensen-Shannon divergence of probability dis-\ntributions outperforms the cosine similarity.', u'In particular,\na newly proposed term distribution taking co-occurrence of\nterms into account gives best results.', u'1.', u'Introduction\n\nIn this paper we consider the problem of \ufb01nding the\nset of most prominent topics in a collection of documents.', u'Since we will not start with a given list of topics, we treat\nthe problem of identifying and characterizing a topic as an\nintegral part of the task.', u'As a consequence, we cannot rely\non a training set or other forms of external knowledge, but\nhave to get by with the information contained in the col-\nlection itself.', u'The approach we will follow consists of two\nsteps.', u'First we extract a list of the most informative key-\nwords.', u'Subsequently we try to identify clusters of keywords\nfor which we will de\ufb01ne a center, which we take as the rep-\nresentation of a topic.', u'We found that this works fairly well\nin an evaluation with Wikipedia articles in which we com-\npared human de\ufb01ned topic categories with keyword clus-\nters.', u'Clustering of (key)words requires a similarity or distance\nmeasure between words.', u'In this paper we consider distance\nmeasures between words that are based on the statistical dis-\ntribution of words in a corpus of texts.', u'The focus of is to \ufb01nd\na measure that yields good clustering results.', u'The organization of this paper is as follows.', u'In section\n2 we discuss related work and the related problem of key-\nword extraction.', u'In section 3 we discuss distance functions\nand introduce different probability distributions needed to\n\nde\ufb01ne them.', u'We end the section with a brief description\nof the clustering algorithm used for evaluation.', u'Finally, in\nsection 4 we present an evaluation of topic detection on a\nWikipedia corpus using clustering of keywords with differ-\nent distance measures.', u'2.', u'Related Work\n\nMuch work has been done on automatic text categoriza-\ntion.', u'Most of this work is concerned with the assignment\nof texts onto a (small) set of given categories.', u'In many\ncases some form of machine learning is used to train an\nalgorithm on a set of manually categorized documents.', u'A\nlot of work has been done on clustering of texts (See e.g.', u'[14] and [12]) which has already found practical applica-\ntions like the clustering of search results (see e.g.', u'http:\n//clusty.com/).', u'The topic of the clusters remains usually implicit in these\napproaches, though it would of course be possible to apply\nany keyword extraction algorithm to the resulting clusters\nin order to \ufb01nd characteristic terms.', u'Like the work presented in this paper, Li and Yaman-\nishi ([10, 9]) try to \ufb01nd characterizations of topics directly\nby clustering keywords using a statistical similarity mea-\nsure.', u'While very similar in spirit, their similarity measure\nis slightly different from the Jensen-Shannon based similar-\nity measure we use.', u'Moreover, they focus on determining\nthe boundaries and the topic of short paragraphs, while we\ntry to \ufb01nd the predominant overall topic of a whole text.', u'Both our and their work are conceptually related to latent\nsemantic analysis (LSA) [3, 7] and even more so to proba-\nbilistic latent semantic analysis (PLSA) [5, 6] and related\nwork by [15].', u'The input for both our methods and (P)LSA\nis the word occurrence and co-occurrence data of terms.', u'Latent semantic analysis now naturally leads to weighted\nsums of terms, with the crucial difference that in the PLSA\ncase only non negative weights with sum 1 are allowed\nwhich have a natural interpretation as conditional probabil-\nities.', u'In the latent semantic analysis approach the analogue\nof clustering is \ufb01nding (latent) conditional probability dis-\ntributions such that we can decompose the observed word\n\n\x0cdistributions into a few of these latent distributions and a\nsmall \u201cnoisy\u201d remainder.', u'In this decomposition words with\nstrong mutual co-occurrence tend to have the same main la-\ntent components.', u'In our work the word clusters are similarly\nbased on co-occurrence data.', u'This is achieved by comparing\nand clustering distributions of co-occurring terms.', u'Thus,\nthe center of a cluster is the average distribution of the co-\noccurrence distributions, and is in this sense comparable to\na latent component.', u'3.', u'Clustering Keywords\n\nBy clustering we will understand grouping terms, docu-\nments or other items together based on some criterion for\nsimilarity.', u'We will always de\ufb01ne similarity using a distance\nfunction on terms, which is in turn de\ufb01ned as a distance\nbetween distributions associated to (key)words by counting\n(co)occurrences in documents.', u'3.1.', u'Keyword Extraction\n\nWe will represent a topic by a cluster of keywords.', u'We\ntherefore need a set of keywords for the corpus under con-\nsideration.', u'Note that \ufb01nding a set of keywords for a corpus\nis not the same problem as assigning keywords to a text\nfrom a list of keywords, nor that of \ufb01nding the most char-\nacteristic terms for a given subset of the corpus.', u'The prob-\nlem of \ufb01nding a good set of keywords is similar to that of\ndetermining term weights for indexing documents, and not\nthe main focus of this paper.', u'For our purposes usable re-\nsults can be obtained by selecting the most frequent nouns,\nverbs (without auxiliaries) and proper names and \ufb01ltering\nout words with little discriminative power (see section 4).', u'3.2.', u'Distributions\n\nWe simplify a document to a bag of words, terms or key-\nwords, in the following always called terms.', u'We consider a\ncollection of n term occurrences W .', u'Each term occurrence\nis an instance of exactly one term t in T = {t1 , .', u'.', u'.', u'tm},\noccurrences of term t in d, n(t) = P\nand can be found in exactly one source document d in a\ncollection C = {d1 , .', u'.', u'.', u'dM }.', u'Let n(d, t) be the number of\nber of occurrences of term t, and N (d) = P\nd n(d, t) be the num-\nt n(d, t) the\nnumber of term occurrences in d.\nWe consider the natural probability distributions Q on\nC \xd7 T , a distribution Q on C and q on T that measure the\nprobability to randomly select an occurrence of a term, from\na source document or both\nQ(d, t) = n(d, t)/n on C \xd7 T\nQ(d) = N (d)/n on C\nq(t) = n(t)/n on T\n\nThese distributions are the baseline probability distributions\nfor everything that we will do in the remainder.', u'In addition\nwe have two important conditional probabilities\nQ(d|t) = Qt (d) = n(d, t)/n(t) on C\nq(t|d) = qd (t) = n(d, t)/N (d) on T\nThe suggestive notation Q(d|t) is used for the source distri-\nbution of t as it is the probability that a randomly selected\noccurrence of term t has source d. Similarly, q(t|d), the\nterm distribution of d is the probability that a randomly se-\nlected term occurrence from document d is an instance of\nterm t. Various other probability distributions on C \xd7 T ,\nC and T that we will consider will be denoted by P, P , p\nrespectively, dressed with various sub and superscripts.', u'Distributions of Co-occurring Terms\n\nThe setup in the previous section allows us to set up a\nMarkov chain on the set of documents and terms which will\nallow us to propagate probability distributions from terms\nto document and vice versa.', u'Consider a Markov chain on\nT \u222aC having transitions C \u2192 T with transition probabilities\nQ(d|t) and transitions T \u2192 C with transition probabilities\nq(t|d) only.', u'Given a term distribution p(t) we compute the one step\nMarkov chain evolution.', u'This gives us a document distri-\nbution Pp (d), the probability to \ufb01nd a term occurrence in a\nparticular document given that the term distribution of the\nPp (d) = X\noccurrences is p\nt\nLikewise given a document distribution P (d), the one step\npP (t) = X\nMarkov chain evolution is the term distribution\nq(t|d)P (d).', u'd\nSince P (d) gives the probability to \ufb01nd a term occurrence\nin document d, pP is the P-weighted average of the term\ndistributions in the documents.', u'Combining these, i.e.', u'run-\nning the Markov chain twice, every term distribution gives\nq(t|d)Pp (d) = X\n\xafp(t) = pPp (t) = X\nrise to a new distribution\nt0 ,d\nd\nIn particular starting from the degenerate \u201cknown to be z\u201d\n(\nterm distribution\n1\n0\n\nq(t|d)Q(d|t0 )p(t0 )\n\npz (t) = p(t|z ) =\n\nQ(d|t)p(t).', u'if t = z ,\notherwise.', u',\n\n\x0cq(t|d)Q(d|t0 )pz (t0 ) = X\n\xafpz (t) = X\nthe distribution of co-occurring terms \xafpz then is\nq(t|d)Q(d|z ).', u'd,t0\nd\nThis distribution is the weighted average of the term distri-\nbutions of documents containing z where the weight is the\nprobability Q(d|z ) that a term occurrence in d is an occur-\nrence of z .', u'Note that the probability measure \xafpz is very similar to\nthe setup in [10, section 3].', u'The difference is that we keep\ntrack of the density of a keyword in a document rather than\njust the mere occurrence or non occurrence of a keyword in\na document.', u'This difference is particularly relevant for long\ndocuments where a word may occur with very low density,\nyet because it contains many words have a signi\ufb01cant con-\ntribution to the mean word distribution.', u'Unfortunately \xafpz is\nexpensive to compute.', u'3.3.', u'Distance Measures\n\nAn effective way to de\ufb01ne \u201csimilarity\u201d between two ele-\nments is through a metric d(i, j ) between the elements i, j\nsatisfying the usual axioms of nonnegativity, identity of in-\ndiscernables and triangle inequality.', u'Two elements are more\nsimilar if they are closer.', u'For this purpose any monotone in-\ncreasing function of a metric will suf\ufb01ce and we will call\nsuch a function a distance function.', u'For clustering we use a hierarchical top-down method,\nthat requires that in each step the center of each cluster is\ncomputed.', u'Thus our choice of distance function is restricted\nto distances de\ufb01ned on a space allowing us to compute a\ncenter and distances between keywords and this center.', u'In\nparticular we cannot use popular similarity measures like\nthe Jaccard coef\ufb01cient.', u'In the following we will compare results with four dif-\nferent distance functions for keywords t and s: (a) the co-\nsine similarity of the document distribution Qt and Qs con-\nsidered as vectors on the document space, (b) the cosine\nsimilarity of the vectors of tf.idf values of keywords, (c) the\nJensen-Shannon divergence between the document distri-\nbutions Qt and Qs and (d) the Jensen-Shannon divergence\nbetween the term distributions, \xafpt and \xafps .', u'The cosine similarity of two terms t and s is de\ufb01ned as\nq(cid:0)P\nt (d)(cid:1) (cid:0)P\ns (d)(cid:1) .', u'\u03a3d\u2208C Qt (d)Qs (d)\nd\u2208C Q2\nd\u2208C Q2\nSince the arccos of this similarity function is a proper met-\nric, (1 \u2212 cos)(arccos(cos sim(t, s))) = 1 \u2212 cos sim(t, s) is\na distance function.', u'The Jensen-Shannon divergence or information radius\n[11, 4] between two distributions p and q is de\ufb01ned as\n1\n1\n2 D(p||m) +\n2 D(q ||m)\nJSD(p||q) =\n\ncos simtf (t, s) =\n\nwhere m = 1/2(p + q) is the mean distribution and D(p||q)\nis the relative entropy or Kullback-Leibler divergence be-\n(cid:19)\n(cid:18) pi\nnX\ntween p and q which is de\ufb01ned by\nqi\ni=1\n\nD(p||q) =\n\npi log\n\n.', u'Since the square root of the Jensen Shannon divergence is a\nproper metric [4], we have two distances\n\nJSD simdoc (t, s) = JSD(Qt , Qs )\n\nand\n\nJSD simterm (t, s) = JSD( \xafps , \xafpt )\n\n3.4.', u'Clustering Method\n\nWe have used the induced bisecting k-means clustering\nalgorithm as described by [1], which is based on the stan-\ndard bisecting k-means algorithm, see e.g.', u'[14].', u'An infor-\nmal description of the method is as follows.', u'We start by\nselecting two elements that have the largest distance, which\nwe use as the seeds for two clusters.', u'Next all other items\nare assigned to the cluster closest to one of the two seeds.', u'After all items have been assigned to a cluster, the centers of\nboth clusters are computed.', u'Here we need a representation\nof items that naturally allows to de\ufb01ne a center which typi-\ncally is not an item proper but a weighted sum of items.', u'The\nnew centers serve as new seeds for \ufb01nding two clusters and\nthe process is repeated until the two centers are converged\nup to some prede\ufb01ned precision.', u'We have now found two\nclusters.', u'If the diameter of a cluster is larger than a speci\ufb01ed\nthreshold value, the whole procedure is applied recursively\nto that cluster.', u'The algorithm therefore \ufb01nds a binary tree\nof clusters.', u'We also experimented with agglomerative hierarchical\nclustering, especially the single link algorithm.', u'First results\nsuggested that this approach performed not as well as the k-\nmeans algorithm, in line with similar \ufb01ndings by [14] and\n[12].', u'Since our focus is to \ufb01nd an optimal distance measure\nand not to \ufb01nd the best clustering algorithm, we did not\ninvestigate the agglomerative methods in more detail.', u'4.', u'Evaluation\n\n4.1.', u'Implementation\n\nTo test and compare the different strategies we have com-\npiled a small corpus of Dutch Wikipedia articles consisting\nof 758 documents.', u'In the analysis phase, 118099 term oc-\ncurrences, and 26373 unique terms were found.', u'The arti-\ncles were taken from 8 Wikipedia categories: space\ufb02ight,\n\n\x0cpainting, architecture, trees, monocots, aviation, pop mu-\nsic, charadriiformes.', u'Categories were selected for subjec-\ntive similarity, like space\ufb02ight and aviation, and subjective\ndissimilarity like pop music and monocots.', u'Articles are\nequally distributed over the categories, but articles in some\ncategories are signi\ufb01cantly longer than in others.', u'Moreover,\nhomogeneity and speci\ufb01ty of articles differs signi\ufb01cantly\nbetween categories.', u'To determine a set of relevant keywords we have selected\nthe most frequent content words and \ufb01ltered out a number\nof overly general terms.', u'The latter was done by requiring\nthat a keyword has to be different enough from the back-\nground distribution q , as measured by the Kullback-Leibler\ndivergence.', u'We used a cutoff D( \xafpt ||q) > 1 bit, that turned\nout to give decent results.', u'Before extracting keywords we do some preprocessing\nusing the GAT E\u2013framework [2].', u'The main analysis steps\nare:\nlemmatization, multiword lookup and named entity\nrecognition.', u'Lemmatization and tagging is done using the\nTreetagger [13].', u'Tagging allows us to distinguish content\nwords from function words.', u'After lemmatization all in-\n\ufb02ected forms of verbs, nouns and adjectives are mapped\nonto their lexical form, substantially reducing the variation\nin the input data.', u'For multiword lookup we used article\ntitles from the Dutch Wikipedia, since it is reasonable to as-\nsume that each title represents a single concept.', u'Finally,\nsome simple rules are applied to identify proper names.', u'While some of the components are language dependent, all\nof the components are available for a number of languages\nwithin the GAT E\u2013framework.', u'We selected the 160 most frequent keywords ful\ufb01lling\nthis criterion and clustered them with the induced bisecting\nk-means algorithm from section 3.4 using different distance\nmeasures.', u'4.2.', u'Results\n\nTo evaluate the implemented topic detection methods,\nwe have compared the results with topics known to be\npresent in the collection.', u'We benchmarked against the 8\nselected Wikipedia topics of the collection.', u'Of course, it\nis conceivable that the collection has more topics that au-\ntomatic methods might recognize as well.', u'To de\ufb01ne a ref-\nerence clustering, we have clustered the 160 selected key-\n9 }, one\n1 , \xb7 \xb7 \xb7 , c\u2217\nwords into a set of 9 categories C \u2217 = {c\u2217\n0 , c\u2217\nfor each Wikipedia category and a rest cluster c\u2217\n0 , using the\nfollowing method.', u'For each of the 8 Wikipedia categories\nc\u2217\ni we compute the distribution qc\u2217\nof words in the docu-\nments belonging to c\u2217\ni\n0 = q .', u'We assign a term\ni and we let qc\u2217\nz to cluster c\u2217 if c\u2217 = argminc\u2217\u2208C D(qc\u2217 || \xafpz ).', u'Following [8], we now compare with the set of clusters\nC of keywords found using the algorithm in section 3.4,\ndifferent distance measures and different diameters.', u'For\n\n.', u'F (c, c\u2217 ) =\n\neach cluster c \u2208 C and cluster c\u2217 \u2208 C \u2217 we de\ufb01ne a re-\ncall measure rec(c, c\u2217 ) = |c \u2229 c\u2217 |/|c\u2217 |, a precision measure\nprec(c, c\u2217 ) = |c \u2229 c\u2217 |/|c\u2217 | and an F value\nrec(c, c\u2217 )prec(c, c\u2217 )\n2 (rec(c, c\u2217 ) + prec(c, c\u2217 ))\n1\nLet F (c\u2217 ) = maxc\u2208C F (c, c\u2217 ) be the F -value of the best\nF = X\n\ufb01tting found cluster and \ufb01nally de\ufb01ne the overall F-value\n|c\u2217 |P\nc\u2217\u2208C \u2217 |c\u2217 | F (c\u2217 )\nc\u2217\u2208C \u2217\nA value of F = 1 therefore means that for each Wikipedia\nand rest category the topic detection algorithm found a cor-\nresponding cluster.', u'Since the clustering algorithm can produce clusters of\ndifferent size, the quality of the result depends on the only\ninput parameter of the algorithm, the threshold value to split\nup a cluster.', u'Its quality (in terms of overall F-value) is com-\npared between distance functions by varying the number\nof clusters as controlled by varying the threshold values.', u'Since there might be several clusters with exactly the same\nlargest distance between two elements, we do not \ufb01nd val-\nues for each number of clusters.', u'In particular, for cos simtf\nand JSD simdoc there are no values that yield a clustering\ninto less than 14 and 11 clusters, resp.', u'In the case of co-\nsine similarity there are in each of these clusters even two\nkeywords with completely orthogonal document distribu-\ntions.', u'The overall F-values for clustering with the different\nsimilarities are given in Figure 1.', u'Cosine similarity with\ntf.idf vectors performed in between direct cosine similarity\nand the Jensen-Shannon distance with the document distri-\nbution\nThe rest category in the reference categorization is not a\nreal cluster or category of keywords.', u'Moreover, this cate-\ngory is about three times as large as the average size of the\nother categories.', u'We have therefore tested how well the 8\npositively motivated categories are detected, and computed\nthe overall F-values averaging only over these 8 categories.', u'The results are given in Figure 2.', u'Results for the cosine sim-\nilarity with tf.idf vector were slightly better than clustering\nwith JSD simdoc\n\n5.', u'Conclusions\n\nThe experimental results suggest that topic identi\ufb01cation\nby clustering a set of keywords works fairly well, using ei-\nther of the investigated similarity measures.', u'In the present\nexperiment a recently proposed distribution of terms asso-\nciated with a keyword clearly gives best results, but compu-\ntation of the distribution is relatively expensive.', u'The reason\nfor this is the fact that co-occurrence of terms is (implicitly)\ntaken into account.', u'The document distribution for terms,\n\n\x0cFigure 1.', u'Overall F-Measure for clustering\nbased on 9 categories\n\nFigure 2.', u'Overall F-Measure for clustering\nbased on 8 categories\n\nthat is the base of the other measures, tend to be very sparse\nvectors, since for a given document most words will not oc-\ncur at all in that document.', u'In the distribution used for the\nJSD simdoc this problem alleviated by a kind of \u2019intelligent\nsmoothing\u2019 or spreading values from one term to frequently\nco-occurring terms.', u'Acknowledgements\n\nThis work is part of the MultimediaN project1 sponsored\nby the Dutch government under contract BSIK 03031.', u'References\n\n[1] F. Archetti, P. Campanelli, E. Fersini, and E. Messina.', u'A\nhierarchical document clustering environment based on the\ninduced bisecting k-means.', u'In H. L. Larsen, G. Pasi, D. O.\nArroyo, T. Andreasen, and H. Christiansen, editors, FQAS,\nvolume 4027 of Lecture Notes in Computer Science, pages\n257\u2013269.', u'Springer, 2006.', u'[2] H. Cunningham, D. Maynard, K. Bontcheva, and V. Tablan.', u'GATE: A Framework and Graphical Development Environ-\nIn Proc.', u'of\nment for Robust NLP Tools and Applications.', u'the 40th Anniversary Meeting of the Association for Compu-\ntational Linguistics (ACL\u201902), pages 168\u2013175, Philadelphia,\nJuly 2002.', u'[3] S. T. Dumais, G. W. Furnas, T. K. Landauer, S. Deerwester,\nand R. Harshman.', u'Using latent semantic analysis to improve\naccess to textual information.', u'In CHI \u201988: Proceedings of the\nSIGCHI conference on Human factors in computing systems,\npages 281\u2013285, New York, NY, USA, 1988.', u'ACM.', u'1 http://www.multimedian.nl\n\nIn\n\n[4] B. Fuglede and F. Topsoe.', u'Jensen-shannon divergence and\nhilbert space embedding.', u'In Proc.', u'of the Internat.', u'Symposium\non Information Theory, 2004, pages 31\u2013, 2004.', u'[5] T. Hofmann.', u'Probabilisitic latent semantic analysis.', u'UAI99: Uncertainty in arti\ufb01cial intelligence, 1999.', u'[6] T. Hofmann.', u'Unsupervised learning by probabilistic latent\nsemantic analysis.', u'Machine Learning, 42(1-2):177\u2013196, Jan-\nuary 2001.', u'[7] T. Landauer, P. Foltz, and D. Laham.', u'Introduction to latent\nsemantic analysis.', u'Discourse Processes, 25:259\u2013284, 1998.', u'[8] B. Larsen and C. Aone.', u'Fast and effective text mining us-\ning linear-time document clustering.', u'In KDD, pages 16\u201322,\n1999.', u'[9] H. Li and K. Yamanishi.', u'Text classi\ufb01cation using esc-based\nstochastic decision lists.', u'Inf.', u'Process.', u'Manage., 38(3):343\u2013\n361, 2002.', u'[10] H. Li and K. Yamanishi.', u'Topic analysis using a \ufb01nite mixture\nmodel.', u'Inf.', u'Process.', u'Manage., 39(4):521\u2013541, 2003.', u'[11] C. D. Manning and H. Sch\xfctze.', u'Foundations of Statistical\nNatural Language Processing.', u'The MIT Press, Cambridge,\nMassachusetts, 1999.', u'[12] S. Meyer zu Eissen and B. Stein.', u'Analysis of clustering\nalgorithms for web-based search.', u'In D. Karagiannis and\nU. Reimer, editors, PAKM, volume 2569 of Lecture Notes\nin Computer Science, pages 168\u2013178.', u'Springer, 2002.', u'[13] H. Schmid.', u'Probabilistic part-of-speech tagging using deci-\nsion trees.', u'In International Conference on New Methods in\nLanguage Processing, Manchester, UK, 1994. unknown.', u'[14] M. Steinbach, G. Karypis, and V. Kumar.', u'A comparison of\ndocument clustering techniques.', u'In Proceedings of Workshop\non Text Mining, 6th ACM SIGKDD International Conference\non Data Mining (KDD\u201900), pages 109\u2013110, August 20\u201323\n2000.', u'[15] L. Zhang, X. Wu, and Y. Yu.', u'Emergent semantics from folk-\nsonomies: A quantitative study.', u'4090:168\u2013186, 2006.\n\n\x0c\n']
